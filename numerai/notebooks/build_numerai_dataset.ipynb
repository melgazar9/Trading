{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "prostate-framework",
   "metadata": {},
   "source": [
    "# This notebook builds the stock market / numerai dataset using dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-triumph",
   "metadata": {
    "id": "breeding-triumph"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "amber-cabin",
   "metadata": {
    "id": "amber-cabin"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RU-UGOk2rA8w",
   "metadata": {
    "id": "RU-UGOk2rA8w"
   },
   "source": [
    "### Only run the below if on google colab notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5vQqHDb0qRLd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5vQqHDb0qRLd",
    "outputId": "6731e0c2-99ac-40b0-b86c-5ecf33dbb34f"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "UreNrl9srNXa",
   "metadata": {
    "id": "UreNrl9srNXa"
   },
   "outputs": [],
   "source": [
    "# sys.path.append('/content/gdrive/trading/dev/scripts/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0l6obyRwr9qy",
   "metadata": {
    "id": "0l6obyRwr9qy"
   },
   "outputs": [],
   "source": [
    "# from gdrive.MyDrive.trading.dev.scripts.ML_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fBnjdmz6rOHn",
   "metadata": {
    "id": "fBnjdmz6rOHn"
   },
   "source": [
    "### Only run if on local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "continuous-chemical",
   "metadata": {
    "id": "continuous-chemical"
   },
   "outputs": [],
   "source": [
    "os.chdir('../..') # local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "chief-death",
   "metadata": {
    "id": "chief-death"
   },
   "outputs": [],
   "source": [
    "os.environ['NUMEXPR_MAX_THREADS'] = '32'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "through-contract",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from dev.scripts.ML_utils import * # run if on local machine\n",
    "from dev.scripts.numerai_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-insured",
   "metadata": {
    "id": "norwegian-insured"
   },
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-conclusion",
   "metadata": {},
   "source": [
    "#### Read in the numerai keys via config parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dynamic-piece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['numerai/numerai_keys.ini']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read('numerai/numerai_keys.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "logical-stage",
   "metadata": {
    "id": "logical-stage"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.01 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "DOWNLOAD_NUMERAI_COMPETITION_DATA = False\n",
    "USE_NUMERAI_COMPETITION_DATA = False\n",
    "\n",
    "DF_NUMERAI_COMP_TRAIN_PATH = 'C:/Users/Matt/trading/numerai/data/numerai_dataset_255/numerai_training_data.csv' # local\n",
    "\n",
    "napi = numerapi.SignalsAPI(config['KEYS']['NUMERAI_PUBLIC_KEY'], config['KEYS']['NUMERAI_SECRET_KEY'])\n",
    "\n",
    "# download data\n",
    "if DOWNLOAD_NUMERAI_COMPETITION_DATA:\n",
    "\n",
    "    # napi = numerapi.NumerAPI(NUMERAI_PUBLIC_KEY, NUMERAI_SECRET_KEY)\n",
    "    napi.download_current_dataset(unzip=True)\n",
    "\n",
    "    if USE_NUMERAI_COMPETITION_DATA:\n",
    "        df_numerai_comp = dd.read_csv(DF_NUMERAI_COMP_TRAIN_PATH).compute()\n",
    "        df_numerai_comp.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZC3OYstYxhEB",
   "metadata": {
    "id": "ZC3OYstYxhEB"
   },
   "source": [
    "## Load in eligible tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "Jw1AlEWtxT6w",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jw1AlEWtxT6w",
    "outputId": "4ba6e5c5-67d3-4db5-d090-e4baf38373ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of eligible tickers: 5431\n",
      "Number of eligible tickers in map: 5431\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>bloomberg_ticker</th>\n",
       "      <th>yahoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5429</th>\n",
       "      <td>ZYXI</td>\n",
       "      <td>ZYXI US</td>\n",
       "      <td>ZYXI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5430</th>\n",
       "      <td>ZZZ.</td>\n",
       "      <td>ZZZ CN</td>\n",
       "      <td>ZZZ.TO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ticker bloomberg_ticker   yahoo\n",
       "5429   ZYXI          ZYXI US    ZYXI\n",
       "5430   ZZZ.           ZZZ CN  ZZZ.TO"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eligible_tickers = pd.Series(napi.ticker_universe(), name='ticker')\n",
    "print(f\"Number of eligible tickers: {len(eligible_tickers)}\")\n",
    "ticker_map = pd.read_csv('https://numerai-signals-public-data.s3-us-west-2.amazonaws.com/signals_ticker_map_w_bbg.csv')\n",
    "ticker_map = ticker_map[ticker_map['bloomberg_ticker'].isin(eligible_tickers)]\n",
    "print(f\"Number of eligible tickers in map: {len(ticker_map)}\")\n",
    "ticker_map.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-dialogue",
   "metadata": {},
   "source": [
    "#### Remove null tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "through-classroom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tickers before: (5431, 3)\n",
      "tickers after: (5380, 3)\n"
     ]
    }
   ],
   "source": [
    "valid_tickers = [i for i in ticker_map['yahoo']\n",
    "     if not pd.isnull(i)\n",
    "     and not str(i).lower()=='nan' \\\n",
    "     and not str(i).lower()=='null' \\\n",
    "]\n",
    "\n",
    "print('tickers before:', ticker_map.shape) # before removing bad tickers\n",
    "ticker_map = ticker_map[ticker_map['yahoo'].isin(valid_tickers)]\n",
    "print('tickers after:', ticker_map.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-netscape",
   "metadata": {},
   "source": [
    "## Download yahoo finance data in the expected numerai format using the yfinance library\n",
    "Yahoo Finance wrappers: https://github.com/ranaroussi/yfinance and https://pypi.org/project/yfinance/. <br>\n",
    "This takes ~2 hours on a single-thread\n",
    "\n",
    "### Convert the yahoo df to a dask df\n",
    "- If we don't do this the computation will be very slow. There are ~20 million rows of daily data alone. <br>\n",
    "- Once I add in intraday data and create additional features, it is necessary to use a lazy computation such as dask or spark. <br>\n",
    "\n",
    "- Lastly, we'll merge in the numerai target variable and save the ddf as a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "latter-confirmation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.23 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>yahoo_ticker</th>\n",
       "      <th>adj_close_1d</th>\n",
       "      <th>close_1d</th>\n",
       "      <th>high_1d</th>\n",
       "      <th>low_1d</th>\n",
       "      <th>open_1d</th>\n",
       "      <th>volume_1d</th>\n",
       "      <th>adj_close_1h_0</th>\n",
       "      <th>adj_close_1h_1</th>\n",
       "      <th>...</th>\n",
       "      <th>volume_1h_15</th>\n",
       "      <th>volume_1h_16</th>\n",
       "      <th>volume_1h_17</th>\n",
       "      <th>volume_1h_18</th>\n",
       "      <th>volume_1h_19</th>\n",
       "      <th>volume_1h_20</th>\n",
       "      <th>volume_1h_21</th>\n",
       "      <th>volume_1h_22</th>\n",
       "      <th>volume_1h_23</th>\n",
       "      <th>bloomberg_ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17616892</th>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>ZURN.SW</td>\n",
       "      <td>406.200012</td>\n",
       "      <td>406.200012</td>\n",
       "      <td>410.799988</td>\n",
       "      <td>405.899994</td>\n",
       "      <td>409.00</td>\n",
       "      <td>1.617723e+12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>60604.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZURN SW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17616893</th>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>15.290000</td>\n",
       "      <td>15.290000</td>\n",
       "      <td>15.420000</td>\n",
       "      <td>14.860000</td>\n",
       "      <td>15.38</td>\n",
       "      <td>2.858690e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>100865.0</td>\n",
       "      <td>8730.0</td>\n",
       "      <td>33090.0</td>\n",
       "      <td>19464.0</td>\n",
       "      <td>49338.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZYXI US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17616894</th>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>15.290000</td>\n",
       "      <td>15.290000</td>\n",
       "      <td>15.420000</td>\n",
       "      <td>14.860000</td>\n",
       "      <td>15.38</td>\n",
       "      <td>2.858690e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>100865.0</td>\n",
       "      <td>8730.0</td>\n",
       "      <td>33090.0</td>\n",
       "      <td>19464.0</td>\n",
       "      <td>49338.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZYXI US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17616895</th>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>ZZZ.TO</td>\n",
       "      <td>31.879999</td>\n",
       "      <td>31.879999</td>\n",
       "      <td>32.240002</td>\n",
       "      <td>31.629999</td>\n",
       "      <td>31.98</td>\n",
       "      <td>1.107286e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>6800.0</td>\n",
       "      <td>3943.0</td>\n",
       "      <td>4128.0</td>\n",
       "      <td>6819.0</td>\n",
       "      <td>6188.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZZZ CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17616896</th>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>ZZZ.TO</td>\n",
       "      <td>31.879999</td>\n",
       "      <td>31.879999</td>\n",
       "      <td>32.240002</td>\n",
       "      <td>31.629999</td>\n",
       "      <td>31.98</td>\n",
       "      <td>1.107286e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>6800.0</td>\n",
       "      <td>3943.0</td>\n",
       "      <td>4128.0</td>\n",
       "      <td>6819.0</td>\n",
       "      <td>6188.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZZZ CN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 153 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               date yahoo_ticker  adj_close_1d    close_1d     high_1d  \\\n",
       "17616892 2021-04-06      ZURN.SW    406.200012  406.200012  410.799988   \n",
       "17616893 2021-04-06         ZYXI     15.290000   15.290000   15.420000   \n",
       "17616894 2021-04-06         ZYXI     15.290000   15.290000   15.420000   \n",
       "17616895 2021-04-06       ZZZ.TO     31.879999   31.879999   32.240002   \n",
       "17616896 2021-04-06       ZZZ.TO     31.879999   31.879999   32.240002   \n",
       "\n",
       "              low_1d  open_1d     volume_1d  adj_close_1h_0  adj_close_1h_1  \\\n",
       "17616892  405.899994   409.00  1.617723e+12             NaN             NaN   \n",
       "17616893   14.860000    15.38  2.858690e+05             NaN             NaN   \n",
       "17616894   14.860000    15.38  2.858690e+05             NaN             NaN   \n",
       "17616895   31.629999    31.98  1.107286e+09             NaN             NaN   \n",
       "17616896   31.629999    31.98  1.107286e+09             NaN             NaN   \n",
       "\n",
       "          ...  volume_1h_15  volume_1h_16  volume_1h_17  volume_1h_18  \\\n",
       "17616892  ...       60604.0           NaN           NaN           NaN   \n",
       "17616893  ...      100865.0        8730.0       33090.0       19464.0   \n",
       "17616894  ...      100865.0        8730.0       33090.0       19464.0   \n",
       "17616895  ...        6800.0        3943.0        4128.0        6819.0   \n",
       "17616896  ...        6800.0        3943.0        4128.0        6819.0   \n",
       "\n",
       "          volume_1h_19  volume_1h_20  volume_1h_21  volume_1h_22  \\\n",
       "17616892           NaN           NaN           NaN           NaN   \n",
       "17616893       49338.0           NaN           NaN           NaN   \n",
       "17616894       49338.0           NaN           NaN           NaN   \n",
       "17616895        6188.0           NaN           NaN           NaN   \n",
       "17616896        6188.0           NaN           NaN           NaN   \n",
       "\n",
       "          volume_1h_23  bloomberg_ticker  \n",
       "17616892           NaN           ZURN SW  \n",
       "17616893           NaN           ZYXI US  \n",
       "17616894           NaN           ZYXI US  \n",
       "17616895           NaN            ZZZ CN  \n",
       "17616896           NaN            ZZZ CN  \n",
       "\n",
       "[5 rows x 153 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "DOWNLOAD_YAHOO_DATA = False\n",
    "if DOWNLOAD_YAHOO_DATA:\n",
    "    ddf_yahoo = dd.from_pandas(download_yfinance_data(list(ticker_map['yahoo']), start='2006-01-01')) # all yahoo tickers\n",
    "else:\n",
    "    DF_YAHOO_FILEPATH = 'data/yfinance/df_yahoo_2021-04-07.pq'\n",
    "    NPARTITIONS=16\n",
    "    if DF_YAHOO_FILEPATH.lower().endswith('pq') or DF_YAHOO_FILEPATH.lower().endswith('parquet'):\n",
    "        ddf_yahoo = dd.read_parquet(DF_YAHOO_FILEPATH,\n",
    "                                    npartitions=NPARTITIONS)\n",
    "    elif DF_YAHOO_FILEPATH.lower().endswith('feather'):\n",
    "        ddf_yahoo = dd.from_pandas(delayed(feather.read_dataframe)('data/yfinance/df_yahoo_2021-04-06.feather').compute(),\n",
    "                                   npartitions=NPARTITIONS)\n",
    "\n",
    "ddf_yahoo.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "front-vermont",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowMemoryError",
     "evalue": "malloc of size 1321268416 failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matt\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\dask\\base.py\u001b[0m in \u001b[0;36mcompute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[0mdask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \"\"\"\n\u001b[1;32m--> 283\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matt\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\dask\\base.py\u001b[0m in \u001b[0;36mcompute\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    563\u001b[0m         \u001b[0mpostcomputes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dask_postcompute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 565\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    566\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrepack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpostcomputes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matt\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\dask\\threaded.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(dsk, result, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m                 \u001b[0mpools\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mthread\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     results = get_async(\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matt\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\dask\\local.py\u001b[0m in \u001b[0;36mget_async\u001b[1;34m(apply_async, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, **kwargs)\u001b[0m\n\u001b[0;32m    485\u001b[0m                         \u001b[0m_execute_task\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Re-execute locally\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 487\u001b[1;33m                         \u001b[0mraise_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    488\u001b[0m                 \u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworker_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"cache\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matt\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\dask\\local.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(exc, tb)\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matt\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\dask\\local.py\u001b[0m in \u001b[0;36mexecute_task\u001b[1;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mtask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtask_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_execute_task\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m         \u001b[0mid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matt\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\dask\\core.py\u001b[0m in \u001b[0;36m_execute_task\u001b[1;34m(arg, cache, dsk)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;31m# temporaries by their reference count and can execute certain\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;31m# operations in-place.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_execute_task\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mishashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matt\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\dask\\dataframe\\io\\parquet\\core.py\u001b[0m in \u001b[0;36mread_parquet_part\u001b[1;34m(fs, func, meta, part, columns, index, kwargs)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m         dfs = [\n\u001b[0m\u001b[0;32m    384\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtoolz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpart\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matt\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\dask\\dataframe\\io\\parquet\\core.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         dfs = [\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtoolz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpart\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m         ]\n",
      "\u001b[1;32mc:\\users\\matt\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\dask\\dataframe\\io\\parquet\\arrow.py\u001b[0m in \u001b[0;36mread_partition\u001b[1;34m(cls, fs, piece, columns, index, categories, partitions, filters, schema, **kwargs)\u001b[0m\n\u001b[0;32m    622\u001b[0m                     \u001b[0marrow_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marrow_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 624\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_arrow_table_to_pandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrow_table\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    625\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m         \u001b[1;31m# For pyarrow.dataset api, need to convert partition columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matt\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\dask\\dataframe\\io\\parquet\\arrow.py\u001b[0m in \u001b[0;36m_arrow_table_to_pandas\u001b[1;34m(cls, arrow_table, categories, **kwargs)\u001b[0m\n\u001b[0;32m   1578\u001b[0m         \u001b[0m_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"use_threads\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ignore_metadata\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1580\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0marrow_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategories\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1582\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matt\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pyarrow\\array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._PandasConvertible.to_pandas\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matt\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pyarrow\\table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table._to_pandas\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matt\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pyarrow\\pandas_compat.py\u001b[0m in \u001b[0;36mtable_to_blockmanager\u001b[1;34m(options, table, categories, ignore_metadata, types_mapper)\u001b[0m\n\u001b[0;32m    790\u001b[0m     \u001b[0m_check_data_column_metadata_consistency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deserialize_column_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_columns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_indexes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m     \u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_table_to_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mext_columns_dtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m     \u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matt\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pyarrow\\pandas_compat.py\u001b[0m in \u001b[0;36m_table_to_blocks\u001b[1;34m(options, block_table, categories, extension_columns)\u001b[0m\n\u001b[0;32m   1129\u001b[0m     \u001b[1;31m# Convert an arrow table to Block from the internal pandas API\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblock_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1131\u001b[1;33m     result = pa.lib.table_to_blocks(options, block_table, categories,\n\u001b[0m\u001b[0;32m   1132\u001b[0m                                     list(extension_columns.keys()))\n\u001b[0;32m   1133\u001b[0m     return [_reconstruct_block(item, columns, extension_columns)\n",
      "\u001b[1;32mc:\\users\\matt\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pyarrow\\table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.table_to_blocks\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matt\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pyarrow\\error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowMemoryError\u001b[0m: malloc of size 1321268416 failed"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_tmp = ddf_yahoo.compute()\n",
    "\n",
    "print(df_tmp.shape)\n",
    "print(df_tmp.dropna().shape)\n",
    "print(df_tmp.dropna(axis=1).shape)\n",
    "print(df_tmp[[i for i in df_tmp.columns if i.endswith('d')]].dropna().shape)\n",
    "del df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "solar-check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 153 entries, date to bloomberg_ticker\n",
      "dtypes: datetime64[ns](1), object(2), float64(150)"
     ]
    }
   ],
   "source": [
    "ddf_yahoo.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-macro",
   "metadata": {},
   "source": [
    "### Map the yahoo tickers to bloomberg tickers in the ddf_yahoo\n",
    "Set to True if reading data - I already saved the bloomberg ticker in the dumped parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "vanilla-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bloomberg_yahoo_tickermap(df,\n",
    "                              ticker_map_dict = dict(zip(ticker_map['yahoo'],\\\n",
    "                                                         ticker_map['bloomberg_ticker']))):\n",
    "    \n",
    "    df.loc[:, 'bloomberg_ticker'] = df['yahoo_ticker'].map(ticker_map_dict)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "funded-regulation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>yahoo_ticker</th>\n",
       "      <th>adj_close_1d</th>\n",
       "      <th>close_1d</th>\n",
       "      <th>high_1d</th>\n",
       "      <th>low_1d</th>\n",
       "      <th>open_1d</th>\n",
       "      <th>volume_1d</th>\n",
       "      <th>adj_close_1h_0</th>\n",
       "      <th>adj_close_1h_1</th>\n",
       "      <th>...</th>\n",
       "      <th>volume_1h_15</th>\n",
       "      <th>volume_1h_16</th>\n",
       "      <th>volume_1h_17</th>\n",
       "      <th>volume_1h_18</th>\n",
       "      <th>volume_1h_19</th>\n",
       "      <th>volume_1h_20</th>\n",
       "      <th>volume_1h_21</th>\n",
       "      <th>volume_1h_22</th>\n",
       "      <th>volume_1h_23</th>\n",
       "      <th>bloomberg_ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17616892</th>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>ZURN.SW</td>\n",
       "      <td>406.200012</td>\n",
       "      <td>406.200012</td>\n",
       "      <td>410.799988</td>\n",
       "      <td>405.899994</td>\n",
       "      <td>409.00</td>\n",
       "      <td>1.617723e+12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>60604.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZURN SW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17616893</th>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>15.290000</td>\n",
       "      <td>15.290000</td>\n",
       "      <td>15.420000</td>\n",
       "      <td>14.860000</td>\n",
       "      <td>15.38</td>\n",
       "      <td>2.858690e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>100865.0</td>\n",
       "      <td>8730.0</td>\n",
       "      <td>33090.0</td>\n",
       "      <td>19464.0</td>\n",
       "      <td>49338.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZYXI US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17616894</th>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>15.290000</td>\n",
       "      <td>15.290000</td>\n",
       "      <td>15.420000</td>\n",
       "      <td>14.860000</td>\n",
       "      <td>15.38</td>\n",
       "      <td>2.858690e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>100865.0</td>\n",
       "      <td>8730.0</td>\n",
       "      <td>33090.0</td>\n",
       "      <td>19464.0</td>\n",
       "      <td>49338.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZYXI US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17616895</th>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>ZZZ.TO</td>\n",
       "      <td>31.879999</td>\n",
       "      <td>31.879999</td>\n",
       "      <td>32.240002</td>\n",
       "      <td>31.629999</td>\n",
       "      <td>31.98</td>\n",
       "      <td>1.107286e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>6800.0</td>\n",
       "      <td>3943.0</td>\n",
       "      <td>4128.0</td>\n",
       "      <td>6819.0</td>\n",
       "      <td>6188.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZZZ CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17616896</th>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>ZZZ.TO</td>\n",
       "      <td>31.879999</td>\n",
       "      <td>31.879999</td>\n",
       "      <td>32.240002</td>\n",
       "      <td>31.629999</td>\n",
       "      <td>31.98</td>\n",
       "      <td>1.107286e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>6800.0</td>\n",
       "      <td>3943.0</td>\n",
       "      <td>4128.0</td>\n",
       "      <td>6819.0</td>\n",
       "      <td>6188.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZZZ CN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 153 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               date yahoo_ticker  adj_close_1d    close_1d     high_1d  \\\n",
       "17616892 2021-04-06      ZURN.SW    406.200012  406.200012  410.799988   \n",
       "17616893 2021-04-06         ZYXI     15.290000   15.290000   15.420000   \n",
       "17616894 2021-04-06         ZYXI     15.290000   15.290000   15.420000   \n",
       "17616895 2021-04-06       ZZZ.TO     31.879999   31.879999   32.240002   \n",
       "17616896 2021-04-06       ZZZ.TO     31.879999   31.879999   32.240002   \n",
       "\n",
       "              low_1d  open_1d     volume_1d  adj_close_1h_0  adj_close_1h_1  \\\n",
       "17616892  405.899994   409.00  1.617723e+12             NaN             NaN   \n",
       "17616893   14.860000    15.38  2.858690e+05             NaN             NaN   \n",
       "17616894   14.860000    15.38  2.858690e+05             NaN             NaN   \n",
       "17616895   31.629999    31.98  1.107286e+09             NaN             NaN   \n",
       "17616896   31.629999    31.98  1.107286e+09             NaN             NaN   \n",
       "\n",
       "          ...  volume_1h_15  volume_1h_16  volume_1h_17  volume_1h_18  \\\n",
       "17616892  ...       60604.0           NaN           NaN           NaN   \n",
       "17616893  ...      100865.0        8730.0       33090.0       19464.0   \n",
       "17616894  ...      100865.0        8730.0       33090.0       19464.0   \n",
       "17616895  ...        6800.0        3943.0        4128.0        6819.0   \n",
       "17616896  ...        6800.0        3943.0        4128.0        6819.0   \n",
       "\n",
       "          volume_1h_19  volume_1h_20  volume_1h_21  volume_1h_22  \\\n",
       "17616892           NaN           NaN           NaN           NaN   \n",
       "17616893       49338.0           NaN           NaN           NaN   \n",
       "17616894       49338.0           NaN           NaN           NaN   \n",
       "17616895        6188.0           NaN           NaN           NaN   \n",
       "17616896        6188.0           NaN           NaN           NaN   \n",
       "\n",
       "          volume_1h_23  bloomberg_ticker  \n",
       "17616892           NaN           ZURN SW  \n",
       "17616893           NaN           ZYXI US  \n",
       "17616894           NaN           ZYXI US  \n",
       "17616895           NaN            ZZZ CN  \n",
       "17616896           NaN            ZZZ CN  \n",
       "\n",
       "[5 rows x 153 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SET_BLOOMBERG_TICKERS_AS_INDEX = False\n",
    "if SET_BLOOMBERG_TICKERS_AS_INDEX:\n",
    "    ddf_yahoo = ddf_yahoo.map_partitions(bloomberg_yahoo_tickermap)\n",
    "\n",
    "ddf_yahoo.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-indianapolis",
   "metadata": {},
   "source": [
    "### Save df_yahoo to a feather file for faster loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "coral-priest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "SAVE_DF_YAHOO_TO_FEATHER = False\n",
    "SAVE_DF_YAHOO_TO_PARQUET = False\n",
    "\n",
    "DDF_YAHOO_OUTPATH = 'data/yfinance/df_yahoo_' + str(datetime.datetime.today().date())\n",
    "if SAVE_DF_YAHOO_TO_FEATHER:\n",
    "    ddf_yahoo.reset_index().to_feather(DDF_YAHOO_OUTPATH + '.feather')\n",
    "if SAVE_DF_YAHOO_TO_PARQUET:\n",
    "    dd.to_parquet(ddf_yahoo,\n",
    "                  path=DDF_YAHOO_OUTPATH + '.pq'#,\n",
    "#                   engine='fastparquet', # fails on windows\n",
    "#                   storage_options={'key':key, 'secret':secret} # used to store on server\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-salad",
   "metadata": {},
   "source": [
    "### Load in the numerai targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "african-volleyball",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.56 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bloomberg_ticker</th>\n",
       "      <th>friday_date</th>\n",
       "      <th>data_type</th>\n",
       "      <th>target</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4294278</th>\n",
       "      <td>ZYXI US</td>\n",
       "      <td>20210319</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2021-03-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4294279</th>\n",
       "      <td>ZZZ CN</td>\n",
       "      <td>20210319</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2021-03-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bloomberg_ticker  friday_date   data_type  target       date\n",
       "4294278          ZYXI US     20210319  validation    0.75 2021-03-19\n",
       "4294279           ZZZ CN     20210319  validation    0.50 2021-03-19"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# targets_address = 'https://numerai-signals-public-data.s3-us-west-2.amazonaws.com/signals_train_val.csv' # old\n",
    "targets_address = 'https://numerai-signals-public-data.s3-us-west-2.amazonaws.com/signals_train_val_bbg.csv'\n",
    "targets = pd.read_csv(targets_address)\\\n",
    "    .assign(date = lambda df: pd.to_datetime(df['friday_date'], format='%Y%m%d'))\n",
    "targets.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "comparative-field",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50    2148478\n",
       "0.25     858391\n",
       "0.75     857948\n",
       "1.00     214798\n",
       "0.00     214665\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-exemption",
   "metadata": {},
   "source": [
    "### Merge targets into ddf_yahoo\n",
    "- From an inner join on `['date', 'bloomberg_ticker']` we lose about 85% of rows. <br>\n",
    "- If we drop rows with NAs we have 0 rows left no matter what. <br>\n",
    "- The best bet seems to be an outer join without dropping NA rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "headed-credits",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.83 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(153, 17616897)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ddf_yahoo.shape[1], ddf_yahoo.shape[0].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "asian-yorkshire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.12 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>yahoo_ticker</th>\n",
       "      <th>adj_close_1d</th>\n",
       "      <th>close_1d</th>\n",
       "      <th>high_1d</th>\n",
       "      <th>low_1d</th>\n",
       "      <th>open_1d</th>\n",
       "      <th>volume_1d</th>\n",
       "      <th>adj_close_1h_0</th>\n",
       "      <th>adj_close_1h_1</th>\n",
       "      <th>...</th>\n",
       "      <th>volume_1h_18</th>\n",
       "      <th>volume_1h_19</th>\n",
       "      <th>volume_1h_20</th>\n",
       "      <th>volume_1h_21</th>\n",
       "      <th>volume_1h_22</th>\n",
       "      <th>volume_1h_23</th>\n",
       "      <th>bloomberg_ticker</th>\n",
       "      <th>friday_date</th>\n",
       "      <th>data_type</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>180084</th>\n",
       "      <td>2021-03-19</td>\n",
       "      <td>ZURN.SW</td>\n",
       "      <td>394.899994</td>\n",
       "      <td>394.899994</td>\n",
       "      <td>396.899994</td>\n",
       "      <td>392.700012</td>\n",
       "      <td>394.299988</td>\n",
       "      <td>1122367.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZURN SW</td>\n",
       "      <td>20210319</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180085</th>\n",
       "      <td>2021-03-19</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.150000</td>\n",
       "      <td>15.320000</td>\n",
       "      <td>15.650000</td>\n",
       "      <td>547200.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>26440.0</td>\n",
       "      <td>125466.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZYXI US</td>\n",
       "      <td>20210319</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180086</th>\n",
       "      <td>2021-03-19</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.150000</td>\n",
       "      <td>15.320000</td>\n",
       "      <td>15.650000</td>\n",
       "      <td>547200.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>26440.0</td>\n",
       "      <td>125466.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZYXI US</td>\n",
       "      <td>20210319</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180087</th>\n",
       "      <td>2021-03-19</td>\n",
       "      <td>ZZZ.TO</td>\n",
       "      <td>31.730000</td>\n",
       "      <td>31.730000</td>\n",
       "      <td>31.885000</td>\n",
       "      <td>30.879999</td>\n",
       "      <td>31.270000</td>\n",
       "      <td>138800.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>12237.0</td>\n",
       "      <td>25319.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZZZ CN</td>\n",
       "      <td>20210319</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180088</th>\n",
       "      <td>2021-03-19</td>\n",
       "      <td>ZZZ.TO</td>\n",
       "      <td>31.730000</td>\n",
       "      <td>31.730000</td>\n",
       "      <td>31.885000</td>\n",
       "      <td>30.879999</td>\n",
       "      <td>31.270000</td>\n",
       "      <td>138800.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>12237.0</td>\n",
       "      <td>25319.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZZZ CN</td>\n",
       "      <td>20210319</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 156 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date yahoo_ticker  adj_close_1d    close_1d     high_1d  \\\n",
       "180084 2021-03-19      ZURN.SW    394.899994  394.899994  396.899994   \n",
       "180085 2021-03-19         ZYXI     16.100000   16.100000   16.150000   \n",
       "180086 2021-03-19         ZYXI     16.100000   16.100000   16.150000   \n",
       "180087 2021-03-19       ZZZ.TO     31.730000   31.730000   31.885000   \n",
       "180088 2021-03-19       ZZZ.TO     31.730000   31.730000   31.885000   \n",
       "\n",
       "            low_1d     open_1d  volume_1d  adj_close_1h_0  adj_close_1h_1  \\\n",
       "180084  392.700012  394.299988  1122367.0             NaN             NaN   \n",
       "180085   15.320000   15.650000   547200.0             NaN             NaN   \n",
       "180086   15.320000   15.650000   547200.0             NaN             NaN   \n",
       "180087   30.879999   31.270000   138800.0             NaN             NaN   \n",
       "180088   30.879999   31.270000   138800.0             NaN             NaN   \n",
       "\n",
       "        ...  volume_1h_18  volume_1h_19  volume_1h_20  volume_1h_21  \\\n",
       "180084  ...           NaN           NaN           NaN           NaN   \n",
       "180085  ...       26440.0      125466.0           NaN           NaN   \n",
       "180086  ...       26440.0      125466.0           NaN           NaN   \n",
       "180087  ...       12237.0       25319.0           NaN           NaN   \n",
       "180088  ...       12237.0       25319.0           NaN           NaN   \n",
       "\n",
       "        volume_1h_22  volume_1h_23  bloomberg_ticker  friday_date   data_type  \\\n",
       "180084           NaN           NaN           ZURN SW     20210319  validation   \n",
       "180085           NaN           NaN           ZYXI US     20210319  validation   \n",
       "180086           NaN           NaN           ZYXI US     20210319  validation   \n",
       "180087           NaN           NaN            ZZZ CN     20210319  validation   \n",
       "180088           NaN           NaN            ZZZ CN     20210319  validation   \n",
       "\n",
       "        target  \n",
       "180084    0.50  \n",
       "180085    0.75  \n",
       "180086    0.75  \n",
       "180087    0.50  \n",
       "180088    0.50  \n",
       "\n",
       "[5 rows x 156 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ddf_yahoo = dd.merge(ddf_yahoo, targets, on=['date', 'bloomberg_ticker'], how='inner')\n",
    "ddf_yahoo.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "remarkable-queensland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2623095, 156)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf_yahoo.shape[0].compute(), ddf_yahoo.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-halifax",
   "metadata": {},
   "source": [
    "### First iteration - drop rows where the daily prices are NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "specified-paraguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# df_yahoo_reduced = df_yahoo[df_yahoo.index.isin(df_yahoo[[i for i in df_yahoo.columns if i.endswith('d')]].dropna().index)]\n",
    "# print(df_yahoo_reduced.shape)\n",
    "\n",
    "# df_yahoo_reduced.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "mineral-cooling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_suffix_nas(df, col_suffix='1d', id_cols=['date', 'bloomberg_ticker']):\n",
    "    \n",
    "    df_ids = df[[col for col in df.columns \\\n",
    "                 if col.endswith(col_suffix) \\\n",
    "                 or col in id_cols]\\\n",
    "               ].dropna()[id_cols].isin(df[id_cols])\n",
    "    \n",
    "    df = df[df[id_cols].isin(df_ids[id_cols])]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "grave-arcade",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_yahoo = ddf_yahoo.map_partitions(drop_suffix_nas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_yahoo.map_partitions(drop_suffix_nas).shape[0].compute(), ddf_yahoo.map_partitions(drop_suffix_nas).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-observation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-corpus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-harvard",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-prairie",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-instrumentation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-passenger",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-vanilla",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-emperor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-protein",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-slope",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-highlight",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-screen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-evaluation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-anger",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-robin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-projector",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-commerce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "SET_BLOOMBERG_TICKERS_AS_INDEX = True\n",
    "if SET_BLOOMBERG_TICKERS_AS_INDEX:\n",
    "    df_yahoo.reset_index(inplace=True)\n",
    "    df_yahoo.loc[:, 'bloomberg_ticker'] = df_yahoo['ticker'].map(dict(zip(ticker_map['yahoo'], ticker_map['bloomberg_ticker'])))\n",
    "    df_yahoo.set_index(['date', 'ticker'], inplace=True)\n",
    "df_yahoo.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-biography",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rolling_features(df,\\\n",
    "                            rolling_params,\\\n",
    "                            rolling_fn,\\\n",
    "                            ewm_params,\\\n",
    "                            ewm_fn,\\\n",
    "                            rolling_cols = 'all_numeric',\\\n",
    "                            ewm_cols = 'all_numeric',\\\n",
    "                            join_method='outer',\\\n",
    "                            groupby_cols = None,\\\n",
    "                            copy=True):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    __________\n",
    "    groupby_cols : list or str cols to group by before applying rolling transformations\n",
    "        example: pass groupby_cols to the stacked ticker numerai dataset, but not a wide df \n",
    "    rolling_cols : cols to apply rolling_fn to\n",
    "    ewm_cols : cols to apply ewm_fn to\n",
    "    rolling_params : dict params passed to df.rolling()\n",
    "    rolling_fn : str called from df.rolling().rolling_fn (e.g. df.rolling.mean() is called with getattr)\n",
    "    ewm_params : dict params passed to df.ewm()\n",
    "    ewm_fn : str called from df.ewm().ewm_fn (e.g. df.ewm.mean() is called with getattr)\n",
    "    join_method : str 'inner', 'outer', 'left', or 'right' - how to join the dfs\n",
    "    copy : bool whether or not to make a copy of the df\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if copy: df = df.copy()\n",
    "    \n",
    "    if rolling_cols.lower() == 'all_numeric':\n",
    "        rolling_cols = list(df.select_dtypes(include=np.number).columns)\n",
    "    if ewm_cols.lower() == 'all_numeric':\n",
    "        ewm_cols = list(df.select_dtypes(include=np.number).columns)\n",
    "    \n",
    "    lag_dfs_lst = []\n",
    "    \n",
    "    if groupby_cols is None:\n",
    "        # rolling\n",
    "        lag_dfs_lst.append(getattr(df[rolling_cols].rolling(**rolling_params), rolling_fn)().add_suffix('_rolling_' + rolling_fn))\n",
    "        \n",
    "        # ewm\n",
    "        lag_dfs_lst.append(getattr(df[ewm_cols].ewm(**ewm_params), ewm_fn)().add_suffix('_ewm_' + ewm_fn))\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        if isinstance(groupby_cols, list):\n",
    "            assert(len(groupby_cols) == len(set(groupby_cols))), 'There are duplicates in groupby_cols!'\n",
    "            rolling_cols_to_select = [i for i in list(set(groupby_cols + rolling_cols)) if i in df.columns] # could be index name\n",
    "            ewm_cols_to_select = [i for i in list(set(groupby_cols + ewm_cols)) if i in df.columns] # could be index name\n",
    "        elif isinstance(groupby_cols, str):\n",
    "            rolling_cols_to_select = [i for i in list(set([groupby_cols] + rolling_cols)) if i in df.columns]\n",
    "            ewm_cols_to_select = [i for i in list(set([groupby_cols] + ewm_cols)) if i in df.columns]\n",
    "        else:\n",
    "            raise('Input param groupby_cols is not a list, string, or None!')\n",
    "        \n",
    "        # rolling\n",
    "        lag_dfs_lst.append(\n",
    "            df[rolling_cols_to_select].\\\n",
    "            groupby(groupby_cols).\\\n",
    "            apply(lambda x: getattr(x.rolling(**rolling_params), rolling_fn)()).\\\n",
    "            add_suffix('_rolling_' + rolling_fn)\\\n",
    "        )\n",
    "        \n",
    "        # ewm\n",
    "        lag_dfs_lst.append(\n",
    "            df[ewm_cols_to_select].\\\n",
    "            groupby(groupby_cols).\\\n",
    "            apply(lambda x: getattr(x.ewm(**ewm_params), ewm_fn)()).\\\n",
    "            add_suffix('_ewm_' + ewm_fn)\\\n",
    "        )\n",
    "\n",
    "    df_lag = reduce(lambda x, y: pd.merge(x, y, how=join_method, left_index=True, right_index=True), lag_dfs_lst)    \n",
    "    \n",
    "    df = pd.merge(df, df_lag, how=join_method, left_index=True, right_index=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-potato",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yahoo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-exhibition",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yahoo = create_rolling_features(df_yahoo,\\\n",
    "                                   rolling_params={'window':30},\\\n",
    "                                   rolling_fn='mean',\\\n",
    "                                   ewm_params={'com':.5},\\\n",
    "                                   ewm_fn='mean',\\\n",
    "                                   rolling_cols = 'all_numeric',\\\n",
    "                                   ewm_cols = 'all_numeric',\\\n",
    "                                   join_method='outer',\\\n",
    "                                   groupby_cols = 'ticker')\n",
    "df_yahoo.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yahoo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-medicaid",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets['date'] = pd.to_datetime(targets['friday_date'], format='%Y%m%d')\n",
    "targets.set_index(['date', 'ticker'], inplace=True)\n",
    "targets.index.names = ['date', 'ticker']\n",
    "\n",
    "targets.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-conflict",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.merge(df_yahoo, targets, how='outer', left_index=True, right_index=True)\n",
    "df_full.drop('friday_date', axis=1, inplace=True)\n",
    "df_full.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-flight",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-acting",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_yahoo, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-ceremony",
   "metadata": {},
   "source": [
    "### There are a lot of missing targets. What do we do with them?\n",
    "- This becomes a semi-supervised learning problem since there is likely predictive information where there is no numerai target <br>\n",
    "- To fill them in, I'm going to take an educated guess and say that Numerai's targets are created based on profitable up moves in the market. <br>\n",
    "- The target they created is likely the following multi-class groups: **strong-short**, **short**, **no-trade**, **buy**, **strong-buy** - Let's find out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-torture",
   "metadata": {},
   "source": [
    "#### First get the tickers available in the target df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "assumed-satellite",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bloomberg_ticker</th>\n",
       "      <th>friday_date</th>\n",
       "      <th>data_type</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4294278</th>\n",
       "      <td>ZYXI US</td>\n",
       "      <td>20210319</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4294279</th>\n",
       "      <td>ZZZ CN</td>\n",
       "      <td>20210319</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bloomberg_ticker  friday_date   data_type  target\n",
       "4294278          ZYXI US     20210319  validation    0.75\n",
       "4294279           ZZZ CN     20210319  validation    0.50"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# targets_address = 'https://numerai-signals-public-data.s3-us-west-2.amazonaws.com/signals_train_val.csv' # old\n",
    "targets_address = 'https://numerai-signals-public-data.s3-us-west-2.amazonaws.com/signals_train_val_bbg.csv'\n",
    "targets = pd.read_csv(targets_address)\n",
    "\n",
    "targets.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "residential-cleaners",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3931"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers_with_target = list(set(\\\n",
    "                               list(set(ticker_map['bloomberg_ticker'].unique().tolist()).\\\n",
    "                                    intersection(targets['ticker'].unique().tolist())) + \\\n",
    "                               list(set(ticker_map['yahoo'].unique().tolist()).\\\n",
    "                                    intersection(targets['ticker'].unique().tolist()))\\\n",
    "                              ))\n",
    "len(tickers_with_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-consistency",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full[df_full['target'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-mumbai",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-painting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-assault",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-disposal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-delicious",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-doubt",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-penny",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-amendment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-appearance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-solution",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-magic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-roller",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UOFVjPU0yu7A",
   "metadata": {
    "id": "UOFVjPU0yu7A"
   },
   "outputs": [],
   "source": [
    "ticker_groups = full_data.groupby('ticker')\n",
    "\n",
    "#create lagged features, lag 0 is that day's value, lag 1 is yesterday's value, etc\n",
    "num_days = 5\n",
    "for day in range(num_days+1):\n",
    "    full_data[f'RSI_quintile_lag_{day}'] = ticker_groups['RSI_quintile'].transform(lambda group: group.shift(day))\n",
    "full_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Es0O8RKKyu-a",
   "metadata": {
    "id": "Es0O8RKKyu-a"
   },
   "outputs": [],
   "source": [
    "# create difference of the lagged features (change in RSI quintile by day)\n",
    "for day in range(num_days):\n",
    "    full_data[f'RSI_diff_{day}'] = full_data[f'RSI_quintile_lag_{day}'] - full_data[f'RSI_quintile_lag_{day + 1}']\n",
    "    full_data[f'RSI_abs_diff_{day}'] = np.abs(full_data[f'RSI_quintile_lag_{day}'] - full_data[f'RSI_quintile_lag_{day + 1}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xkrsTeFuyvBq",
   "metadata": {
    "id": "xkrsTeFuyvBq"
   },
   "outputs": [],
   "source": [
    "feature_names = [f'RSI_quintile_lag_{num}' for num in range(num_days)] + [f'RSI_diff_{num}' for num in range(num_days)] + [f'RSI_abs_diff_{num}' for num in range(num_days)]\n",
    "print(f'Features for training:\\n {feature_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KahFrrERyvE5",
   "metadata": {
    "id": "KahFrrERyvE5"
   },
   "outputs": [],
   "source": [
    "TARGET_NAME = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dqoFqMzyvIG",
   "metadata": {
    "id": "8dqoFqMzyvIG"
   },
   "outputs": [],
   "source": [
    "# read in Signals targets\n",
    "numerai_targets = 'https://numerai-signals-public-data.s3-us-west-2.amazonaws.com/signals_train_val.csv'\n",
    "targets = pd.read_csv(numerai_targets)\n",
    "targets['date'] = pd.to_datetime(targets['friday_date'], format='%Y%m%d')\n",
    "targets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m0XWeLvZyvLr",
   "metadata": {
    "id": "m0XWeLvZyvLr"
   },
   "outputs": [],
   "source": [
    "# the number of tickers per era has generally increased\n",
    "targets.groupby('date').apply(lambda x: len(x)).plot(kind='line', figsize=(10,4), title='Number of tickers per era')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wqC4bJX2yvO8",
   "metadata": {
    "id": "wqC4bJX2yvO8"
   },
   "outputs": [],
   "source": [
    "# the target classes are imbalanced, but we can treat this like a regression problem\n",
    "targets.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CVF1kpUkyvSe",
   "metadata": {
    "id": "CVF1kpUkyvSe"
   },
   "outputs": [],
   "source": [
    "# the imbalance is consistent across eras with a constant class ratio of: 5%, 20%, 50%, 20%, 5%\n",
    "pivot_target = targets.groupby(['date','target']).apply(lambda x: len(x)).reset_index(1).pivot(columns='target',values=0)\n",
    "pivot_target.iloc[::20].plot(kind='bar', stacked=True, figsize=(9,3), title='Number of tickers in each class per era')\n",
    "\n",
    "stacked_data = pivot_target.apply(lambda x: x/sum(x), axis=1)\n",
    "stacked_data.iloc[::20].plot(kind='bar', stacked=True, figsize=(9,3), title='Proportion of tickers in each class per era')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A7nFiUa-yvXy",
   "metadata": {
    "id": "A7nFiUa-yvXy"
   },
   "outputs": [],
   "source": [
    "targets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dL1psmzPyvbs",
   "metadata": {
    "id": "dL1psmzPyvbs"
   },
   "outputs": [],
   "source": [
    "# merge our feature data with Numerai targets\n",
    "ML_data = pd.merge(full_data.reset_index(), targets, on=['date','ticker']).set_index('date')\n",
    "# print(f'Number of eras in data: {len(ML_data.index.unique())}')\n",
    "\n",
    "# for training and testing we want clean, complete data only\n",
    "ML_data.dropna(inplace=True)\n",
    "ML_data = ML_data[ML_data.index.weekday==4] # ensure we have only fridays\n",
    "ML_data = ML_data[ML_data.index.value_counts() > 200] # drop eras with under 200 observations per era"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UeymnV_nzT5d",
   "metadata": {
    "id": "UeymnV_nzT5d"
   },
   "outputs": [],
   "source": [
    "print(f'Number of eras in data: {len(ML_data.index.unique())}')\n",
    "ML_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zcLutuwHzT8_",
   "metadata": {
    "id": "zcLutuwHzT8_"
   },
   "outputs": [],
   "source": [
    "train_data = ML_data[ML_data['data_type'] == 'train']\n",
    "test_data = ML_data[ML_data['data_type'] == 'validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pWE4J6ihzUAR",
   "metadata": {
    "id": "pWE4J6ihzUAR"
   },
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor()\n",
    "model.fit(train_data[feature_names], train_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JmssRtBKzUDS",
   "metadata": {
    "id": "JmssRtBKzUDS"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,3))\n",
    "plt.bar(feature_names, model.feature_importances_)\n",
    "plt.xticks(rotation=70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "snk6cOZTzUHN",
   "metadata": {
    "id": "snk6cOZTzUHN"
   },
   "outputs": [],
   "source": [
    "PREDICTION_NAME = 'prediction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iwdod193zUKP",
   "metadata": {
    "id": "iwdod193zUKP"
   },
   "outputs": [],
   "source": [
    "train_data[PREDICTION_NAME] = model.predict(train_data[feature_names])\n",
    "test_data[PREDICTION_NAME] = model.predict(test_data[feature_names])\n",
    "\n",
    "#show prediction distribution, most should around the center\n",
    "test_data[PREDICTION_NAME].hist(bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h5OK3cJvzUNw",
   "metadata": {
    "id": "h5OK3cJvzUNw"
   },
   "outputs": [],
   "source": [
    "def score(df):\n",
    "    '''Takes df and calculates spearm correlation from pre-defined cols'''\n",
    "    # method=\"first\" breaks ties based on order in array\n",
    "    return np.corrcoef(\n",
    "        df[TARGET_NAME],\n",
    "        df[PREDICTION_NAME].rank(pct=True, method=\"first\")\n",
    "    )[0,1]\n",
    "\n",
    "def run_analytics(era_scores):\n",
    "    print(f\"Mean Correlation: {era_scores.mean():.4f}\")\n",
    "    print(f\"Median Correlation: {era_scores.median():.4f}\")\n",
    "    print(f\"Standard Deviation: {era_scores.std():.4f}\")\n",
    "    print('\\n')\n",
    "    print(f\"Mean Pseudo-Sharpe: {era_scores.mean()/era_scores.std():.4f}\")\n",
    "    print(f\"Median Pseudo-Sharpe: {era_scores.median()/era_scores.std():.4f}\")\n",
    "    print('\\n')\n",
    "    print(f'Hit Rate (% positive eras): {era_scores.apply(lambda x: np.sign(x)).value_counts()[1]/len(era_scores):.2%}')\n",
    "\n",
    "    era_scores.rolling(10).mean().plot(kind='line', title='Rolling Per Era Correlation Mean', figsize=(15,4))\n",
    "    plt.axhline(y=0.0, color=\"r\", linestyle=\"--\"); plt.show()\n",
    "\n",
    "    era_scores.cumsum().plot(title='Cumulative Sum of Era Scores', figsize=(15,4))\n",
    "    plt.axhline(y=0.0, color=\"r\", linestyle=\"--\"); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1_pLA3_PzURS",
   "metadata": {
    "id": "1_pLA3_PzURS"
   },
   "outputs": [],
   "source": [
    "# spearman scores by era\n",
    "train_era_scores = train_data.groupby(train_data.index).apply(score)\n",
    "test_era_scores = test_data.groupby(test_data.index).apply(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JHPMlABQzUUd",
   "metadata": {
    "id": "JHPMlABQzUUd"
   },
   "outputs": [],
   "source": [
    "#train scores, in-sample and will be significantly overfit\n",
    "run_analytics(train_era_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uFsUxckKzUYj",
   "metadata": {
    "id": "uFsUxckKzUYj"
   },
   "outputs": [],
   "source": [
    "#test scores, out of sample\n",
    "run_analytics(test_era_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t30vuingzUbd",
   "metadata": {
    "id": "t30vuingzUbd"
   },
   "outputs": [],
   "source": [
    "# choose data as of most recent friday\n",
    "last_friday = datetime.now() + relativedelta(weekday=FR(-1))\n",
    "date_string = last_friday.strftime('%Y-%m-%d')\n",
    "\n",
    "live_data = full_data.loc[date_string].copy()\n",
    "live_data.dropna(subset=feature_names, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WEhtlB4DzUfL",
   "metadata": {
    "id": "WEhtlB4DzUfL"
   },
   "outputs": [],
   "source": [
    "print(f\"Number of live tickers to submit: {len(live_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jPmPsAGYztjQ",
   "metadata": {
    "id": "jPmPsAGYztjQ"
   },
   "outputs": [],
   "source": [
    "live_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R2er-yacztmB",
   "metadata": {
    "id": "R2er-yacztmB"
   },
   "outputs": [],
   "source": [
    "live_data[PREDICTION_NAME] = model.predict(live_data[feature_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ISCnCGQDztop",
   "metadata": {
    "id": "ISCnCGQDztop"
   },
   "outputs": [],
   "source": [
    "diagnostic_df = pd.concat([test_data, live_data])\n",
    "diagnostic_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R5pfv7VTztr9",
   "metadata": {
    "id": "R5pfv7VTztr9"
   },
   "outputs": [],
   "source": [
    "diagnostic_df['friday_date'] = diagnostic_df.friday_date.fillna(last_friday.strftime('%Y%m%d')).astype(int)\n",
    "diagnostic_df['data_type'] = diagnostic_df.data_type.fillna('live')\n",
    "diagnostic_df[['ticker','friday_date','data_type','prediction']].reset_index(drop=True).to_csv('example_signal_upload.csv', index=False)\n",
    "diagnostic_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bGWNHsMDztuq",
   "metadata": {
    "id": "bGWNHsMDztuq"
   },
   "outputs": [],
   "source": [
    "# format predictions to match Numerai submission format\n",
    "predictions = live_data[['ticker', PREDICTION_NAME]].copy()\n",
    "\n",
    "# choose account\n",
    "ACCOUNT_NAME = 'ENTER_ACCOUNT_NAME'\n",
    "\n",
    "# write predictions to csv\n",
    "live_data[['ticker', PREDICTION_NAME]].to_csv(f\"{ACCOUNT_NAME} {datetime.now().strftime('%Y%m%d')}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WBoZIk_xztxf",
   "metadata": {
    "id": "WBoZIk_xztxf"
   },
   "outputs": [],
   "source": [
    "def submit_model(account_name):\n",
    "    filename = f\"{account_name} {datetime.now().strftime('%Y%m%d')}.csv\"\n",
    "    model_id = napi.get_models()[f'{account_name}']\n",
    "    submission = napi.upload_predictions(filename, model_id=model_id)\n",
    "    print(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q7M_Y8bYzt07",
   "metadata": {
    "id": "q7M_Y8bYzt07"
   },
   "outputs": [],
   "source": [
    "submit_model(ACCOUNT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pd23tVq_zt3_",
   "metadata": {
    "id": "Pd23tVq_zt3_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80DA0_H-zt7V",
   "metadata": {
    "id": "80DA0_H-zt7V"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ep_UuQiVzt-X",
   "metadata": {
    "id": "ep_UuQiVzt-X"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Yn45a8_ozuBK",
   "metadata": {
    "id": "Yn45a8_ozuBK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_fo8lf2BzuEe",
   "metadata": {
    "id": "_fo8lf2BzuEe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "discrete-corpus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def download_yfinance_data(tickers,\n",
      "                           intervals_to_download=['1d', '1h'],\n",
      "                           num_workers=1,\n",
      "                           join_method='outer',\n",
      "                           max_intraday_lookback_days=363,\n",
      "                           **yfinance_params):\n",
      "    \"\"\"\n",
      "    Parameters\n",
      "    __________\n",
      "\n",
      "    See yfinance.download docs for a detailed description of yfinance parameters\n",
      "\n",
      "    tickers : string separated by space tickers to pass to yfinance.download (e.g. \"AAPL MSFT FB\")\n",
      "    intervals_to_download : list of intervals to download OHLCV data for each stock (e.g. ['1w', '1d', '1h'])\n",
      "    num_workers : number of threads used to download the data\n",
      "        so far only 1 thread is implemented\n",
      "    join_method : can be 'inner', 'left', 'right' or 'outer'\n",
      "        if 'outer' then all dates will be present\n",
      "        if 'left' then all dates from the left most table will be present\n",
      "        if 'right' then all dates from the left most table will be present\n",
      "        if 'inner' then all dates must match for each ticker\n",
      "    **yfinance_params : dict - passed to yfinance.dowload(yfinance_params)\n",
      "\n",
      "    NOTE: passing some intervals return unreliable stock data (e.g. '3mo' returns many NA data points when they should not be NA)\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    yfinance_params2 = yfinance_params.copy() # create a copy for min / hour pulls because the start date can only go back 60 days\n",
      "\n",
      "    if num_workers == 1:\n",
      "\n",
      "        list_of_dfs = []\n",
      "\n",
      "        for i in intervals_to_download:\n",
      "\n",
      "            yfinance_params['interval'] = i\n",
      "\n",
      "            if i.endswith('m') or i.endswith('h'): # min or hr\n",
      "\n",
      "                yfinance_params2['interval'] = i\n",
      "                yfinance_params2['start'] = str(datetime.datetime.today().date() - datetime.timedelta(days=max_intraday_lookback_days))\n",
      "\n",
      "\n",
      "                df_i = yfinance.download(tickers, **yfinance_params2).\\\n",
      "                        stack().\\\n",
      "                        add_suffix('_' + str(i)).\\\n",
      "                        reset_index(level=1).\\\n",
      "                        rename(columns={'level_1' : 'ticker'})\n",
      "\n",
      "                df_i = df_i.pivot_table(index=df_i.index.date, columns = ['ticker', df_i.index.hour]).stack(level=1)\n",
      "                df_i.columns = list(pd.Index([str(e[0]).lower() + '_' + str(e[1]).lower() for e in df_i.columns.tolist()]).str.replace(' ', '_'))\n",
      "\n",
      "            else:\n",
      "                df_i = yfinance.download(tickers, **yfinance_params).\\\n",
      "                        stack().\\\n",
      "                        add_suffix('_' + str(i))\n",
      "\n",
      "                df_i.columns = [col.replace(' ', '_').lower() for col in df_i.columns]\n",
      "\n",
      "            df_i.index.names = ['date', 'ticker']\n",
      "\n",
      "            list_of_dfs.append(df_i)\n",
      "\n",
      "\n",
      "        df_yahoo = reduce(lambda x, y: pd.merge(x, y, how=join_method, left_index=True, right_index=True), list_of_dfs)\n",
      "#         df_yahoo.reset_index(level=1, inplace=True)\n",
      "\n",
      "    else:\n",
      "        return 'multi-threading not implemented yet. Set num_workers to 1.'\n",
      "\n",
      "    return df_yahoo\n"
     ]
    }
   ],
   "source": [
    "import inspect as i\n",
    "import sys\n",
    "sys.stdout.write(i.getsource(download_yfinance_data))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "build_numerai_dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
